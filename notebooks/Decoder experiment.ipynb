{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Names\n",
    "load_pre = '../'\n",
    "bases_name = 'bases.npy'\n",
    "alpha_name = 'batch_alpha.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filter size\n",
    "F = np.load(load_pre + bases_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64)\n",
      "(256, 16)\n"
     ]
    }
   ],
   "source": [
    "components =16\n",
    "embedding = SpectralEmbedding(n_components=components)\n",
    "F_embed = embedding.fit_transform(F)\n",
    "print(F.shape)\n",
    "print(F_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 16)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.concatenate((F, F_embed), axis = 1)\n",
    "mask = np.random.rand(len(data)) < 0.9\n",
    "\n",
    "# load data\n",
    "train_data = data[mask]\n",
    "test_data = data[~mask]\n",
    "\n",
    "#separate train data\n",
    "train_x = train_data[:, F.shape[1]:]\n",
    "train_y = train_data[:, :F.shape[1]]\n",
    "\n",
    "# #separate test data\n",
    "test_x = test_data[:, F.shape[1]:]\n",
    "test_y = test_data[:, :F.shape[1]]\n",
    "(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_116\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_364 (Dense)            (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_366 (Dense)            (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 9,408\n",
      "Trainable params: 9,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'name': 'Adam', 'learning_rate': 0.001, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}\n",
      "Train on 226 samples, validate on 30 samples\n",
      "Epoch 1/150\n",
      "226/226 [==============================] - 3s 11ms/sample - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 2/150\n",
      "226/226 [==============================] - 0s 427us/sample - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 3/150\n",
      "226/226 [==============================] - 0s 489us/sample - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 4/150\n",
      "226/226 [==============================] - 0s 445us/sample - loss: 0.0155 - val_loss: 0.0155\n",
      "Epoch 5/150\n",
      "226/226 [==============================] - 0s 504us/sample - loss: 0.0154 - val_loss: 0.0155\n",
      "Epoch 6/150\n",
      "226/226 [==============================] - 0s 491us/sample - loss: 0.0154 - val_loss: 0.0154\n",
      "Epoch 7/150\n",
      "226/226 [==============================] - 0s 421us/sample - loss: 0.0153 - val_loss: 0.0154\n",
      "Epoch 8/150\n",
      "226/226 [==============================] - 0s 414us/sample - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 9/150\n",
      "226/226 [==============================] - 0s 402us/sample - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 10/150\n",
      "226/226 [==============================] - 0s 410us/sample - loss: 0.0151 - val_loss: 0.0151\n",
      "Epoch 11/150\n",
      "226/226 [==============================] - 0s 402us/sample - loss: 0.0150 - val_loss: 0.0151\n",
      "Epoch 12/150\n",
      "226/226 [==============================] - 0s 401us/sample - loss: 0.0149 - val_loss: 0.0150\n",
      "Epoch 13/150\n",
      "226/226 [==============================] - 0s 421us/sample - loss: 0.0148 - val_loss: 0.0149\n",
      "Epoch 14/150\n",
      "226/226 [==============================] - 0s 352us/sample - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 15/150\n",
      "226/226 [==============================] - 0s 426us/sample - loss: 0.0146 - val_loss: 0.0147\n",
      "Epoch 16/150\n",
      "226/226 [==============================] - 0s 416us/sample - loss: 0.0145 - val_loss: 0.0147\n",
      "Epoch 17/150\n",
      "226/226 [==============================] - 0s 443us/sample - loss: 0.0144 - val_loss: 0.0146\n",
      "Epoch 18/150\n",
      "226/226 [==============================] - 0s 431us/sample - loss: 0.0142 - val_loss: 0.0145\n",
      "Epoch 19/150\n",
      "226/226 [==============================] - 0s 421us/sample - loss: 0.0141 - val_loss: 0.0144\n",
      "Epoch 20/150\n",
      "226/226 [==============================] - 0s 380us/sample - loss: 0.0140 - val_loss: 0.0144\n",
      "Epoch 21/150\n",
      "226/226 [==============================] - 0s 421us/sample - loss: 0.0139 - val_loss: 0.0142\n",
      "Epoch 22/150\n",
      "226/226 [==============================] - 0s 395us/sample - loss: 0.0138 - val_loss: 0.0141\n",
      "Epoch 23/150\n",
      "226/226 [==============================] - 0s 351us/sample - loss: 0.0137 - val_loss: 0.0141\n",
      "Epoch 24/150\n",
      "226/226 [==============================] - 0s 395us/sample - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 25/150\n",
      "226/226 [==============================] - 0s 386us/sample - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 26/150\n",
      "226/226 [==============================] - 0s 420us/sample - loss: 0.0134 - val_loss: 0.0138\n",
      "Epoch 27/150\n",
      "226/226 [==============================] - 0s 398us/sample - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 28/150\n",
      "226/226 [==============================] - 0s 414us/sample - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 29/150\n",
      "226/226 [==============================] - 0s 401us/sample - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 30/150\n",
      "226/226 [==============================] - 0s 426us/sample - loss: 0.0131 - val_loss: 0.0136\n",
      "Epoch 31/150\n",
      "226/226 [==============================] - 0s 428us/sample - loss: 0.0130 - val_loss: 0.0135\n",
      "Epoch 32/150\n",
      "226/226 [==============================] - 0s 432us/sample - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 33/150\n",
      "226/226 [==============================] - 0s 388us/sample - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 34/150\n",
      "226/226 [==============================] - 0s 408us/sample - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 35/150\n",
      "226/226 [==============================] - 0s 427us/sample - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 36/150\n",
      "226/226 [==============================] - 0s 482us/sample - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 37/150\n",
      "226/226 [==============================] - 0s 505us/sample - loss: 0.0126 - val_loss: 0.0132\n",
      "Epoch 38/150\n",
      "226/226 [==============================] - 0s 590us/sample - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 39/150\n",
      "226/226 [==============================] - 0s 629us/sample - loss: 0.0125 - val_loss: 0.0131\n",
      "Epoch 40/150\n",
      "226/226 [==============================] - 0s 670us/sample - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 41/150\n",
      "226/226 [==============================] - 0s 666us/sample - loss: 0.0124 - val_loss: 0.0130\n",
      "Epoch 42/150\n",
      "226/226 [==============================] - 0s 669us/sample - loss: 0.0123 - val_loss: 0.0130\n",
      "Epoch 43/150\n",
      "226/226 [==============================] - 0s 700us/sample - loss: 0.0123 - val_loss: 0.0129\n",
      "Epoch 44/150\n",
      "226/226 [==============================] - 0s 513us/sample - loss: 0.0122 - val_loss: 0.0129\n",
      "Epoch 45/150\n",
      "226/226 [==============================] - 0s 490us/sample - loss: 0.0122 - val_loss: 0.0129\n",
      "Epoch 46/150\n",
      "226/226 [==============================] - 0s 526us/sample - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 47/150\n",
      "226/226 [==============================] - 0s 384us/sample - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 48/150\n",
      "226/226 [==============================] - 0s 387us/sample - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 49/150\n",
      "226/226 [==============================] - 0s 434us/sample - loss: 0.0120 - val_loss: 0.0128\n",
      "Epoch 50/150\n",
      "226/226 [==============================] - 0s 404us/sample - loss: 0.0120 - val_loss: 0.0127\n",
      "Epoch 51/150\n",
      "226/226 [==============================] - 0s 390us/sample - loss: 0.0119 - val_loss: 0.0127\n",
      "Epoch 52/150\n",
      "226/226 [==============================] - 0s 389us/sample - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 53/150\n",
      "226/226 [==============================] - 0s 433us/sample - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 54/150\n",
      "226/226 [==============================] - 0s 454us/sample - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 55/150\n",
      "226/226 [==============================] - 0s 678us/sample - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 56/150\n",
      "226/226 [==============================] - 0s 591us/sample - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 57/150\n",
      "226/226 [==============================] - 0s 420us/sample - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 58/150\n",
      "226/226 [==============================] - 0s 373us/sample - loss: 0.0118 - val_loss: 0.0126\n",
      "Epoch 59/150\n",
      "226/226 [==============================] - 0s 357us/sample - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 60/150\n",
      "226/226 [==============================] - 0s 405us/sample - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 61/150\n",
      "226/226 [==============================] - 0s 461us/sample - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 62/150\n",
      "226/226 [==============================] - 0s 437us/sample - loss: 0.0117 - val_loss: 0.0125\n",
      "Epoch 63/150\n",
      "226/226 [==============================] - 0s 416us/sample - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 64/150\n",
      "226/226 [==============================] - 0s 434us/sample - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 65/150\n",
      "226/226 [==============================] - 0s 404us/sample - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 66/150\n",
      "226/226 [==============================] - 0s 428us/sample - loss: 0.0116 - val_loss: 0.0125\n",
      "Epoch 67/150\n",
      "226/226 [==============================] - 0s 411us/sample - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 68/150\n",
      "226/226 [==============================] - 0s 409us/sample - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/226 [==============================] - 0s 398us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 70/150\n",
      "226/226 [==============================] - 0s 412us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 71/150\n",
      "226/226 [==============================] - 0s 413us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 72/150\n",
      "226/226 [==============================] - 0s 462us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 73/150\n",
      "226/226 [==============================] - 0s 463us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 74/150\n",
      "226/226 [==============================] - 0s 436us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 75/150\n",
      "226/226 [==============================] - 0s 417us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 76/150\n",
      "226/226 [==============================] - 0s 410us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 77/150\n",
      "226/226 [==============================] - 0s 413us/sample - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 78/150\n",
      "226/226 [==============================] - 0s 401us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 79/150\n",
      "226/226 [==============================] - 0s 442us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 80/150\n",
      "226/226 [==============================] - 0s 397us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 81/150\n",
      "226/226 [==============================] - 0s 437us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 82/150\n",
      "226/226 [==============================] - 0s 431us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 83/150\n",
      "226/226 [==============================] - 0s 389us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 84/150\n",
      "226/226 [==============================] - 0s 442us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 85/150\n",
      "226/226 [==============================] - 0s 410us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 86/150\n",
      "226/226 [==============================] - 0s 366us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 87/150\n",
      "226/226 [==============================] - 0s 392us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 88/150\n",
      "226/226 [==============================] - 0s 421us/sample - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 89/150\n",
      "226/226 [==============================] - 0s 401us/sample - loss: 0.0114 - val_loss: 0.0124\n",
      "Epoch 90/150\n",
      "226/226 [==============================] - 0s 416us/sample - loss: 0.0114 - val_loss: 0.0123\n",
      "Epoch 91/150\n",
      "226/226 [==============================] - 0s 386us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 92/150\n",
      "226/226 [==============================] - 0s 453us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 93/150\n",
      "226/226 [==============================] - 0s 426us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 94/150\n",
      "226/226 [==============================] - 0s 407us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 95/150\n",
      "226/226 [==============================] - 0s 431us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 96/150\n",
      "226/226 [==============================] - 0s 399us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 97/150\n",
      "226/226 [==============================] - 0s 431us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 98/150\n",
      "226/226 [==============================] - 0s 409us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 99/150\n",
      "226/226 [==============================] - 0s 429us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 100/150\n",
      "226/226 [==============================] - 0s 416us/sample - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 101/150\n",
      "226/226 [==============================] - 0s 408us/sample - loss: 0.0113 - val_loss: 0.0122\n",
      "Epoch 102/150\n",
      "226/226 [==============================] - 0s 408us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 103/150\n",
      "226/226 [==============================] - 0s 477us/sample - loss: 0.0113 - val_loss: 0.0123\n",
      "Epoch 104/150\n",
      "226/226 [==============================] - 0s 444us/sample - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 105/150\n",
      "226/226 [==============================] - 0s 434us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 106/150\n",
      "226/226 [==============================] - 0s 396us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 107/150\n",
      "226/226 [==============================] - 0s 380us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 108/150\n",
      "226/226 [==============================] - 0s 378us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 109/150\n",
      "226/226 [==============================] - 0s 374us/sample - loss: 0.0112 - val_loss: 0.0122\n",
      "Epoch 110/150\n",
      "226/226 [==============================] - 0s 411us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 111/150\n",
      "226/226 [==============================] - 0s 423us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 112/150\n",
      "226/226 [==============================] - 0s 411us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 113/150\n",
      "226/226 [==============================] - 0s 391us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 114/150\n",
      "226/226 [==============================] - 0s 399us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 115/150\n",
      "226/226 [==============================] - 0s 419us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 116/150\n",
      "226/226 [==============================] - 0s 455us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 117/150\n",
      "226/226 [==============================] - 0s 418us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 118/150\n",
      "226/226 [==============================] - 0s 436us/sample - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 119/150\n",
      "226/226 [==============================] - 0s 423us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 120/150\n",
      "226/226 [==============================] - 0s 471us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 121/150\n",
      "226/226 [==============================] - 0s 426us/sample - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 122/150\n",
      "226/226 [==============================] - 0s 395us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 123/150\n",
      "226/226 [==============================] - 0s 442us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 124/150\n",
      "226/226 [==============================] - 0s 414us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 125/150\n",
      "226/226 [==============================] - 0s 391us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 126/150\n",
      "226/226 [==============================] - 0s 405us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 127/150\n",
      "226/226 [==============================] - 0s 388us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 128/150\n",
      "226/226 [==============================] - 0s 396us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 129/150\n",
      "226/226 [==============================] - 0s 551us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 130/150\n",
      "226/226 [==============================] - 0s 471us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 131/150\n",
      "226/226 [==============================] - 0s 457us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 132/150\n",
      "226/226 [==============================] - 0s 397us/sample - loss: 0.0110 - val_loss: 0.0122\n",
      "Epoch 133/150\n",
      "226/226 [==============================] - 0s 367us/sample - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 134/150\n",
      "226/226 [==============================] - 0s 382us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 135/150\n",
      "226/226 [==============================] - 0s 400us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 136/150\n",
      "226/226 [==============================] - 0s 382us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 137/150\n",
      "226/226 [==============================] - 0s 404us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 138/150\n",
      "226/226 [==============================] - 0s 415us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 139/150\n",
      "226/226 [==============================] - 0s 387us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 140/150\n",
      "226/226 [==============================] - 0s 411us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 141/150\n",
      "226/226 [==============================] - 0s 368us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 142/150\n",
      "226/226 [==============================] - 0s 406us/sample - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 143/150\n",
      "226/226 [==============================] - 0s 427us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 144/150\n",
      "226/226 [==============================] - 0s 419us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 145/150\n",
      "226/226 [==============================] - 0s 484us/sample - loss: 0.0110 - val_loss: 0.0121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/150\n",
      "226/226 [==============================] - 0s 476us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 147/150\n",
      "226/226 [==============================] - 0s 424us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 148/150\n",
      "226/226 [==============================] - 0s 407us/sample - loss: 0.0110 - val_loss: 0.0121\n",
      "Epoch 149/150\n",
      "226/226 [==============================] - 0s 389us/sample - loss: 0.0109 - val_loss: 0.0120\n",
      "Epoch 150/150\n",
      "226/226 [==============================] - 0s 418us/sample - loss: 0.0109 - val_loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# model.add(tf.keras.Input(shape=(16,)))\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(components,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "print(model.optimizer.get_config())\n",
    "history = model.fit(train_x, train_y, epochs=150, shuffle=True, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012042516842484474\n"
     ]
    }
   ],
   "source": [
    "test_loss = model.evaluate(test_x, test_y, verbose=2)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5d08fe54c0>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(test_x)\n",
    "\n",
    "feature = 3\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(predictions[feature, :].reshape([k_sz, k_sz]))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_y[feature, :].reshape([k_sz, k_sz]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
